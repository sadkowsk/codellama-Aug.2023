# code-llama
Impact of Code Llama since release: media coverage, downloads, etc.

Purpose of this ReadMe: review of the 24 Aug. 2023 publication

Full article citation: 

## Overview
tl;dr: 

What to look for in this presentation (discussion Q&A)
1. Q1
2. Q2
### Context

<img width="1090" alt="Screenshot 2023-10-18 at 12 42 15" src="https://github.com/sadkowsk/code-llama/assets/143565317/78775c6e-95df-4f97-9311-53f0a0033510">
"Figure 2: The Code Llama specialization pipeline. The different stages of fine-tuning annotated with the number of tokens seen during training. Infilling-capable models are marked with the â‡„ symbol." (Code Llama, p.3)

### Research Question
* 
### Approach
* (Most important part of the paper?)
### Results
* 
### Critical Analysis
* What was overlooked by the authors? What could have been developed further? Were there any errors? Have others disputed the findings?

## Demonstration

## Quick Start

## Related Links
### Meta Publications
* Llama 2 (18 Jul. 2023): https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
* Code Llama (24 Aug. 2023): https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/
### GitHub Repositories
* Llama: https://github.com/facebookresearch/llama
* Code Llama: https://github.com/facebookresearch/codellama
### Hugging Face Blogs
* Llama 2: https://huggingface.co/blog/llama2
* Code Llama: https://huggingface.co/blog/codellama
### Hugging Face Models
* Code Llama: https://huggingface.co/codellama
